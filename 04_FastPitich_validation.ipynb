{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea801208-e8e3-47d4-b099-ac2e952e4b4a",
   "metadata": {
    "id": "ef75d1d5"
   },
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "321fdfb6-0ce1-46d9-a48f-77f071e2feb3",
   "metadata": {
    "id": "LggELooctXCT",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import IPython.display as ipd\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b12bff96-7ed5-4a06-8e70-3fd7e40b9eec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SPEAKER_ID = \"lukas\"\n",
    "MODEL_NAME = \"tts_en_fastpitch\"\n",
    "\n",
    "WANDB_PROJECT = \"tts-lukas\"\n",
    "WANDB_ENTITY = \"capecape\" # replace with your wandb username or team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "999f424b-7678-4053-8d7d-27414284e179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# which split we are using\n",
    "validation_split_artifact = f'{WANDB_ENTITY}/{WANDB_PROJECT}/lukas_split:latest'\n",
    "\n",
    "# which model\n",
    "model_artifact = f'{WANDB_ENTITY}/{WANDB_PROJECT}/model-2022-12-08_13-54-17:v3'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e5b186-f25d-4f7b-a5b3-66cd7184b2c4",
   "metadata": {
    "id": "c3bdf1ed"
   },
   "source": [
    "## Synthesize Samples from Finetuned Checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b78a58-8ded-4a40-a97d-25a1e7116641",
   "metadata": {
    "id": "f2b46325"
   },
   "source": [
    "Once we have finetuned our FastPitch model, we can synthesize the audio samples for given text using the following inference steps. We use a HiFi-GAN vocoder trained on LJSpeech.\n",
    "\n",
    "We define some helper functions as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37a58e48-1a58-4b1f-8000-acc3f665605e",
   "metadata": {
    "id": "886c91dc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-12-08 14:20:52 optimizers:55] Apex was not found. Using the lamb or fused_adam optimizer will error out.\n",
      "[NeMo W 2022-12-08 14:20:52 experimental:27] Module <class 'nemo.collections.common.tokenizers.text_to_speech.tts_tokenizers.IPATokenizer'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-12-08 14:20:52 experimental:27] Module <class 'nemo.collections.tts.models.radtts.RadTTSModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n"
     ]
    }
   ],
   "source": [
    "from nemo.collections.tts.models import HifiGanModel\n",
    "from nemo.collections.tts.models import FastPitchModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42825d44-9ce0-4ed0-b3b1-3f77a23af3d5",
   "metadata": {},
   "source": [
    "we will load a pretrained model to generate the voice from the spectrogram (we will later fine tune this model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4e9da1c-d2b9-43c7-b54f-8ed2f1297855",
   "metadata": {
    "id": "886c91dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-12-08 14:20:52 cloud:56] Found existing object /home/tcapelle/.cache/torch/NeMo/NeMo_1.14.0rc0/tts_hifigan/e6da322f0f7e7dcf3f1900a9229a7e69/tts_hifigan.nemo.\n",
      "[NeMo I 2022-12-08 14:20:52 cloud:62] Re-using file from: /home/tcapelle/.cache/torch/NeMo/NeMo_1.14.0rc0/tts_hifigan/e6da322f0f7e7dcf3f1900a9229a7e69/tts_hifigan.nemo\n",
      "[NeMo I 2022-12-08 14:20:52 common:912] Instantiating model from pre-trained checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-12-08 14:20:55 modelPT:142] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    dataset:\n",
      "      _target_: nemo.collections.tts.data.datalayers.MelAudioDataset\n",
      "      manifest_filepath: /home/fkreuk/data/train_finetune.txt\n",
      "      min_duration: 0.75\n",
      "      n_segments: 8192\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: true\n",
      "      batch_size: 64\n",
      "      num_workers: 4\n",
      "    \n",
      "[NeMo W 2022-12-08 14:20:55 modelPT:149] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    dataset:\n",
      "      _target_: nemo.collections.tts.data.datalayers.MelAudioDataset\n",
      "      manifest_filepath: /home/fkreuk/data/val_finetune.txt\n",
      "      min_duration: 3\n",
      "      n_segments: 66150\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: false\n",
      "      batch_size: 5\n",
      "      num_workers: 4\n",
      "    \n",
      "[NeMo W 2022-12-08 14:20:55 features:244] Using torch_stft is deprecated and has been removed. The values have been forcibly set to False for FilterbankFeatures and AudioToMelSpectrogramPreprocessor. Please set exact_pad to True as needed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-12-08 14:20:55 features:267] PADDING: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-12-08 14:20:55 features:244] Using torch_stft is deprecated and has been removed. The values have been forcibly set to False for FilterbankFeatures and AudioToMelSpectrogramPreprocessor. Please set exact_pad to True as needed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-12-08 14:20:55 features:267] PADDING: 0\n",
      "[NeMo I 2022-12-08 14:20:58 save_restore_connector:243] Model HifiGanModel was successfully restored from /home/tcapelle/.cache/torch/NeMo/NeMo_1.14.0rc0/tts_hifigan/e6da322f0f7e7dcf3f1900a9229a7e69/tts_hifigan.nemo.\n"
     ]
    }
   ],
   "source": [
    "vocoder = HifiGanModel.from_pretrained(\"tts_hifigan\")\n",
    "vocoder = vocoder.eval().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce759a4-4ec7-486b-86d8-5bf11c61c23c",
   "metadata": {},
   "source": [
    "We can grab the fine tuned models from the `wandb` artifact:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5b1592-aeb6-4240-8847-6944806fcf5a",
   "metadata": {},
   "source": [
    "Let's log the model predictions to `W&B`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "348dbb3b-e91c-425e-b9e2-c72bced7b983",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcapecape\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tcapelle/wandb/nvidia-workshop/wandb/run-20221208_142059-mkd6e0vc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/capecape/tts-lukas/runs/mkd6e0vc\" target=\"_blank\">fast-cosmos-4</a></strong> to <a href=\"https://wandb.ai/capecape/tts-lukas\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/capecape/tts-lukas/runs/mkd6e0vc?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fa178f57640>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(entity=WANDB_ENTITY, project=WANDB_PROJECT, job_type=\"fastptich_validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87e5ede5-727b-43bf-8baf-ad9df8b68e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   2 of 2 files downloaded.  \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact model-2022-12-08_13-54-17:v3, 524.07MB. 1 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 0:0:0.4\n"
     ]
    }
   ],
   "source": [
    "split_artifact = wandb.use_artifact(validation_split_artifact)\n",
    "split_artifact_dir = split_artifact.download()\n",
    "\n",
    "model_artifact = wandb.use_artifact(model_artifact, type='model')\n",
    "model_artifact_dir = model_artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3516b90f-e9db-4e66-b52a-819ae76aedba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ls(path): return list(Path(path).iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63084f5b-29e9-4ad8-a0bb-9c0a8eda7f41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('artifacts/lukas_split:v0/lukas_manifest_train_local.json'),\n",
       " PosixPath('artifacts/lukas_split:v0/lukas_manifest_valid_local.json')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls(split_artifact_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edaa77e5-5ea2-4015-92e4-5fffc70c9cb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('artifacts/model-2022-12-08_13-54-17:v3/model.ckpt')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls(model_artifact_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "832d09cd-2451-48bb-b58a-b82fcbeabd28",
   "metadata": {
    "id": "0a4c986f"
   },
   "outputs": [],
   "source": [
    "def infer(spec_gen_model, vocoder_model, str_input, speaker=None):\n",
    "    \"\"\"\n",
    "    Synthesizes spectrogram and audio from a text string given a spectrogram synthesis and vocoder model.\n",
    "    \n",
    "    Args:\n",
    "        spec_gen_model: Spectrogram generator model (FastPitch in our case)\n",
    "        vocoder_model: Vocoder model (HiFiGAN in our case)\n",
    "        str_input: Text input for the synthesis\n",
    "        speaker: Speaker ID\n",
    "    \n",
    "    Returns:\n",
    "        spectrogram and waveform of the synthesized audio.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        parsed = spec_gen_model.parse(str_input)\n",
    "        if speaker is not None:\n",
    "            speaker = torch.tensor([speaker]).long().to(device=spec_gen_model.device)\n",
    "        spectrogram = spec_gen_model.generate_spectrogram(tokens=parsed, speaker=speaker)\n",
    "        audio = vocoder_model.convert_spectrogram_to_audio(spec=spectrogram)\n",
    "        \n",
    "    if spectrogram is not None:\n",
    "        if isinstance(spectrogram, torch.Tensor):\n",
    "            spectrogram = spectrogram.to('cpu').numpy()\n",
    "        if len(spectrogram.shape) == 3:\n",
    "            spectrogram = spectrogram[0]\n",
    "    if isinstance(audio, torch.Tensor):\n",
    "        audio = audio.to('cpu').numpy()\n",
    "    return spectrogram, audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "072b4ab2-c225-4d40-9b68-c5f5b1b56035",
   "metadata": {
    "id": "8901f88b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artifacts/model-2022-12-08_13-54-17:v3/model.ckpt\n",
      "[NeMo I 2022-12-08 14:21:08 tokenize_and_classify:87] Creating ClassifyFst grammars.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-12-08 14:21:40 experimental:27] Module <class 'nemo_text_processing.g2p.modules.IPAG2P'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-12-08 14:21:41 modules:95] apply_to_oov_word=None, This means that some of words will remain unchanged if they are not handled by any of the rules in self.parse_one_word(). This may be intended if phonemes and chars are both valid inputs, otherwise, you may see unexpected deletions in your input.\n",
      "[NeMo W 2022-12-08 14:21:41 modelPT:142] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    dataset:\n",
      "      _target_: nemo.collections.tts.torch.data.TTSDataset\n",
      "      manifest_filepath: lukas_manifest_train_local.json\n",
      "      sample_rate: 22050\n",
      "      sup_data_path: ./fastpitch_sup_data\n",
      "      sup_data_types:\n",
      "      - align_prior_matrix\n",
      "      - pitch\n",
      "      n_fft: 1024\n",
      "      win_length: 1024\n",
      "      hop_length: 256\n",
      "      window: hann\n",
      "      n_mels: 80\n",
      "      lowfreq: 0\n",
      "      highfreq: 8000\n",
      "      max_duration: null\n",
      "      min_duration: 0.1\n",
      "      ignore_file: null\n",
      "      trim: false\n",
      "      pitch_fmin: 30\n",
      "      pitch_fmax: 512\n",
      "      pitch_norm: true\n",
      "      pitch_mean: 179.0\n",
      "      pitch_std: 57.15\n",
      "      use_beta_binomial_interpolator: true\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: true\n",
      "      batch_size: 16\n",
      "      num_workers: 12\n",
      "      pin_memory: true\n",
      "    \n",
      "[NeMo W 2022-12-08 14:21:41 modelPT:149] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    dataset:\n",
      "      _target_: nemo.collections.tts.torch.data.TTSDataset\n",
      "      manifest_filepath: lukas_manifest_valid_local.json\n",
      "      sample_rate: 22050\n",
      "      sup_data_path: ./fastpitch_sup_data\n",
      "      sup_data_types:\n",
      "      - align_prior_matrix\n",
      "      - pitch\n",
      "      n_fft: 1024\n",
      "      win_length: 1024\n",
      "      hop_length: 256\n",
      "      window: hann\n",
      "      n_mels: 80\n",
      "      lowfreq: 0\n",
      "      highfreq: 8000\n",
      "      max_duration: null\n",
      "      min_duration: null\n",
      "      ignore_file: null\n",
      "      trim: false\n",
      "      pitch_fmin: 30\n",
      "      pitch_fmax: 512\n",
      "      pitch_norm: true\n",
      "      pitch_mean: 179.0\n",
      "      pitch_std: 57.15\n",
      "      use_beta_binomial_interpolator: true\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: false\n",
      "      batch_size: 16\n",
      "      num_workers: 8\n",
      "      pin_memory: true\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-12-08 14:21:41 features:267] PADDING: 1\n"
     ]
    }
   ],
   "source": [
    "last_ckpt = str(ls(model_artifact_dir)[0])\n",
    "print(last_ckpt)\n",
    "\n",
    "spec_model = FastPitchModel.load_from_checkpoint(last_ckpt)\n",
    "spec_model.eval().cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a644740-9cdc-4604-9fdf-4082a5f0bc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_audio(text, speaker_id):\n",
    "    \"Generate MEL and Synth Audio\"\n",
    "    spec, audio = infer(spec_model, vocoder, text, speaker=speaker_id)\n",
    "    return spec, audio.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2b25cc9-0eda-4f99-b48b-a8d14406eb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_speaker_id = 42\n",
    "duration_mins = 5\n",
    "mixing = False\n",
    "original_speaker_id = \"ljspeech\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffe96de4-bd40-4167-a4c0-e799893a65f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio_filepath</th>\n",
       "      <th>text</th>\n",
       "      <th>duration</th>\n",
       "      <th>text_no_preprocessing</th>\n",
       "      <th>text_normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lukas/seg238.wav</td>\n",
       "      <td>is yes, then you really do have a machine lea...</td>\n",
       "      <td>5</td>\n",
       "      <td>is yes, then you really do have a machine lea...</td>\n",
       "      <td>is yes, then you really do have a machine lear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lukas/seg239.wav</td>\n",
       "      <td>excited enough about all the applications of ...</td>\n",
       "      <td>4</td>\n",
       "      <td>excited enough about all the applications of ...</td>\n",
       "      <td>excited enough about all the applications of m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lukas/seg240.wav</td>\n",
       "      <td>videos that explain actually how to build the...</td>\n",
       "      <td>4</td>\n",
       "      <td>videos that explain actually how to build the...</td>\n",
       "      <td>videos that explain actually how to build thes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lukas/seg241.wav</td>\n",
       "      <td>we're going to keep creating these videos so ...</td>\n",
       "      <td>4</td>\n",
       "      <td>we're going to keep creating these videos so ...</td>\n",
       "      <td>we're going to keep creating these videos so y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lukas/seg242.wav</td>\n",
       "      <td>first to know when a new video comes out.</td>\n",
       "      <td>21</td>\n",
       "      <td>first to know when a new video comes out.</td>\n",
       "      <td>first to know when a new video comes out.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     audio_filepath                                               text  \\\n",
       "0  lukas/seg238.wav   is yes, then you really do have a machine lea...   \n",
       "1  lukas/seg239.wav   excited enough about all the applications of ...   \n",
       "2  lukas/seg240.wav   videos that explain actually how to build the...   \n",
       "3  lukas/seg241.wav   we're going to keep creating these videos so ...   \n",
       "4  lukas/seg242.wav          first to know when a new video comes out.   \n",
       "\n",
       "   duration                              text_no_preprocessing  \\\n",
       "0         5   is yes, then you really do have a machine lea...   \n",
       "1         4   excited enough about all the applications of ...   \n",
       "2         4   videos that explain actually how to build the...   \n",
       "3         4   we're going to keep creating these videos so ...   \n",
       "4        21          first to know when a new video comes out.   \n",
       "\n",
       "                                     text_normalized  \n",
       "0  is yes, then you really do have a machine lear...  \n",
       "1  excited enough about all the applications of m...  \n",
       "2  videos that explain actually how to build thes...  \n",
       "3  we're going to keep creating these videos so y...  \n",
       "4          first to know when a new video comes out.  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df = pd.read_json(Path(split_artifact_dir)/f\"{SPEAKER_ID}_manifest_valid_local.json\", lines=True)\n",
    "valid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ed01bee-f3a1-4a55-92a2-c1de863cf793",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = wandb.Table(columns=['Text', 'Real validation audio', f'Audio Speaker {new_speaker_id}', 'Spec'])\n",
    "\n",
    "sample_rate=22050\n",
    "\n",
    "for _, val_record in valid_df.iterrows():\n",
    "    speaker_spec, speaker_audio = generate_audio(val_record['text'], speaker_id=new_speaker_id)\n",
    "    row = [val_record[\"text_no_preprocessing\"],\n",
    "           wandb.Audio(val_record['audio_filepath'], sample_rate=sample_rate), \n",
    "           wandb.Audio(speaker_audio.flatten(), sample_rate=sample_rate),\n",
    "           wandb.Image(speaker_spec)]\n",
    "    table.add_data(*row)\n",
    "\n",
    "wandb.log({\"fastpitch_predictions\": table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44617b0a-096f-4cd6-a264-d2323f7b14c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e081ca40ba3d420aa7c537540e733157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='2.495 MB of 2.496 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.999749â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">fast-cosmos-4</strong>: <a href=\"https://wandb.ai/capecape/tts-lukas/runs/mkd6e0vc\" target=\"_blank\">https://wandb.ai/capecape/tts-lukas/runs/mkd6e0vc</a><br/>Synced 6 W&B file(s), 1 media file(s), 16 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221208_142059-mkd6e0vc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
