{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea801208-e8e3-47d4-b099-ac2e952e4b4a",
   "metadata": {
    "id": "ef75d1d5"
   },
   "source": [
    "## Finetuning FastPitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321fdfb6-0ce1-46d9-a48f-77f071e2feb3",
   "metadata": {
    "id": "LggELooctXCT",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import IPython.display as ipd\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80c8066-a8eb-48c3-bd0d-dbda23094bdb",
   "metadata": {
    "id": "lhhg2wBNtW0r"
   },
   "source": [
    "Let's also download the pretrained checkpoint that we want to finetune from. NeMo will save checkpoints to `~/.cache`, so let's move that to our current directory. \n",
    "\n",
    "*Note: please, check that `home_path` refers to your home folder. Otherwise, change it manually.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12bff96-7ed5-4a06-8e70-3fd7e40b9eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPEAKER_ID = \"9017\"\n",
    "MODEL_NAME = \"tts_en_fastpitch\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b7bc11-2d34-4f4a-99c6-fb8e125bc8a1",
   "metadata": {},
   "source": [
    "Let's download the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f3d986-3b44-4e3a-a0da-88e4a5fa0a9c",
   "metadata": {
    "id": "LggELooctXCT",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nemo.collections.tts.models import FastPitchModel\n",
    "FastPitchModel.from_pretrained(MODEL_NAME);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c62728-0b7f-404b-831d-50d80bd6dcab",
   "metadata": {
    "id": "lhhg2wBNtW0r"
   },
   "source": [
    "We will copy the model to our current working dir for simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed70f0b-6651-4997-b755-e63b93987301",
   "metadata": {
    "id": "LggELooctXCT",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for nemo_file in (Path.home()/\".cache/torch/NeMo/\").glob(f\"**/{MODEL_NAME}_align.nemo\"):\n",
    "    print(f\"Copying {nemo_file} to ./{nemo_file.name}\")\n",
    "    Path(f\"./{nemo_file.name}\").write_bytes(nemo_file.read_bytes())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dac038-c59c-46c3-94e3-15419d0666e2",
   "metadata": {
    "id": "12b5511c"
   },
   "source": [
    "## Train Time!\n",
    "\n",
    "We can now train our model with the following command:\n",
    "\n",
    "**NOTE: This will take about 50 minutes on colab's K80 GPUs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c348d955-988d-47aa-807b-b50d860efe21",
   "metadata": {
    "id": "reY1LV4lwWoq",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!(python fastpitch_finetune.py --config-name=fastpitch_align_v1.05.yaml \\\n",
    "  train_dataset=9017_manifest_train_local.json \\\n",
    "  validation_datasets=9017_manifest_valid_local.json \\\n",
    "  exp_manager.exp_dir=./fastpitch_finetune \\\n",
    "  +exp_manager.create_wandb_logger=True \\\n",
    "  +exp_manager.wandb_logger_kwargs='{project:nemo, job_type:training, log_model:True}' \\\n",
    "  +init_from_nemo_model=./tts_en_fastpitch_align.nemo \\\n",
    "  model.pitch_mean=121.9 model.pitch_std=23.1 \\\n",
    "  model.pitch_fmin=30 model.pitch_fmax=512 \\\n",
    "  trainer.max_steps=1000 \\\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a64ef3-3382-4e4a-b23e-0a1f857f9b96",
   "metadata": {
    "id": "j2svKvd1eMhf"
   },
   "source": [
    "Let's take a closer look at the training command:\n",
    "\n",
    "* `--config-name=fastpitch_align_v1.05.yaml`\n",
    "  * We first tell the script what config file to use.\n",
    "\n",
    "* `train_dataset=./6097_manifest_train_dur_5_mins_local.json \n",
    "  validation_datasets=./6097_manifest_dev_ns_all_local.json \n",
    "  sup_data_path=./fastpitch_sup_data`\n",
    "  * We tell the script what manifest files to train and eval on, as well as where supplementary data is located (or will be calculated and saved during training if not provided).\n",
    "  \n",
    "* `phoneme_dict_path=tts_dataset_files/cmudict-0.7b_nv22.10 \n",
    "heteronyms_path=tts_dataset_files/heteronyms-052722\n",
    "whitelist_path=tts_dataset_files/lj_speech.tsv \n",
    "`\n",
    "  * We tell the script where `phoneme_dict_path`, `heteronyms-052722` and `whitelist_path` are located. These are the additional files we downloaded earlier, and are used in preprocessing the data.\n",
    "  \n",
    "* `exp_manager.exp_dir=./ljspeech_to_6097_no_mixing_5_mins`\n",
    "  * Where we want to save our log files, tensorboard file, checkpoints, and more.\n",
    "\n",
    "* `+init_from_nemo_model=./tts_en_fastpitch_align.nemo`\n",
    "  * We tell the script what checkpoint to finetune from.\n",
    "\n",
    "* `+trainer.max_steps=1000 ~trainer.max_epochs trainer.check_val_every_n_epoch=25`\n",
    "  * For this experiment, we tell the script to train for 1000 training steps/iterations rather than specifying a number of epochs to run. Since the config file specifies `max_epochs` instead, we need to remove that using `~trainer.max_epochs`.\n",
    "\n",
    "* `model.train_ds.dataloader_params.batch_size=24 model.validation_ds.dataloader_params.batch_size=24`\n",
    "  * Set batch sizes for the training and validation data loaders.\n",
    "\n",
    "* `model.n_speakers=1`\n",
    "  * The number of speakers in the data. There is only 1 for now, but we will revisit this parameter later in the notebook.\n",
    "\n",
    "* `model.pitch_mean=121.9 model.pitch_std=23.1 model.pitch_fmin=30 model.pitch_fmax=512`\n",
    "  * For the new speaker, we need to define new pitch hyperparameters for better audio quality.\n",
    "  * These parameters work for speaker 6097 from the Hi-Fi TTS dataset.\n",
    "  * For speaker 92, we suggest `model.pitch_mean=214.5 model.pitch_std=30.9 model.pitch_fmin=80 model.pitch_fmax=512`.\n",
    "  * fmin and fmax are hyperparameters to librosa's pyin function. We recommend tweaking these per speaker.\n",
    "  * After fmin and fmax are defined, pitch mean and std can be easily extracted.\n",
    "\n",
    "* `model.optim.lr=2e-4 ~model.optim.sched model.optim.name=adam`\n",
    "  * For fine-tuning, we lower the learning rate.\n",
    "  * We use a fixed learning rate of 2e-4.\n",
    "  * We switch from the lamb optimizer to the adam optimizer.\n",
    "\n",
    "* `trainer.devices=1 trainer.strategy=null`\n",
    "  * For this notebook, we default to 1 gpu which means that we do not need ddp.\n",
    "  * If you have the compute resources, feel free to scale this up to the number of free gpus you have available.\n",
    "  * Please remove the `trainer.strategy=null` section if you intend on multi-gpu training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9497baa7-e87a-47e6-a4f9-3e631a67f9e1",
   "metadata": {
    "id": "c3bdf1ed"
   },
   "source": [
    "## Synthesize Samples from Finetuned Checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c3b08d-c201-4ba5-b34e-b325eafe11f6",
   "metadata": {
    "id": "f2b46325"
   },
   "source": [
    "Once we have finetuned our FastPitch model, we can synthesize the audio samples for given text using the following inference steps. We use a HiFi-GAN vocoder trained on LJSpeech.\n",
    "\n",
    "We define some helper functions as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e0db1a-0f9a-476f-926a-0f2307de2f65",
   "metadata": {
    "id": "886c91dc"
   },
   "outputs": [],
   "source": [
    "from nemo.collections.tts.models import HifiGanModel\n",
    "from nemo.collections.tts.models import FastPitchModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001113fa-2365-446b-943e-fb4daf83b545",
   "metadata": {},
   "source": [
    "we will load a pretrained model to generate the voice from the spectrogram (we will later fine tune this model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e92db0-cf97-49c1-8632-1a5fe772a3cd",
   "metadata": {
    "id": "886c91dc"
   },
   "outputs": [],
   "source": [
    "vocoder = HifiGanModel.from_pretrained(\"tts_hifigan\")\n",
    "vocoder = vocoder.eval().cuda();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60440ec1-26d6-49ff-991b-c1b7809b6213",
   "metadata": {},
   "source": [
    "We can grab the fine tuned models from the `wandb` artifact:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5531df3a-f71e-478a-8f9f-5933589b0913",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"nemo\", job_type=\"fastptich_validation\")\n",
    "artifact = wandb.use_artifact('capecape/nemo/model-2022-12-06_16-43-23:v0', type='model')\n",
    "artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bb5a72-b5ac-49fb-a384-520705786999",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ls(path): return list(Path(path).iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d421e5-9ecb-4dce-a53a-97270785c644",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls(artifact_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b3158e-2897-4f11-a7b1-765e4e88b04d",
   "metadata": {
    "id": "0a4c986f"
   },
   "outputs": [],
   "source": [
    "def infer(spec_gen_model, vocoder_model, str_input, speaker=None):\n",
    "    \"\"\"\n",
    "    Synthesizes spectrogram and audio from a text string given a spectrogram synthesis and vocoder model.\n",
    "    \n",
    "    Args:\n",
    "        spec_gen_model: Spectrogram generator model (FastPitch in our case)\n",
    "        vocoder_model: Vocoder model (HiFiGAN in our case)\n",
    "        str_input: Text input for the synthesis\n",
    "        speaker: Speaker ID\n",
    "    \n",
    "    Returns:\n",
    "        spectrogram and waveform of the synthesized audio.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        parsed = spec_gen_model.parse(str_input)\n",
    "        if speaker is not None:\n",
    "            speaker = torch.tensor([speaker]).long().to(device=spec_gen_model.device)\n",
    "        spectrogram = spec_gen_model.generate_spectrogram(tokens=parsed, speaker=speaker)\n",
    "        audio = vocoder_model.convert_spectrogram_to_audio(spec=spectrogram)\n",
    "        \n",
    "    if spectrogram is not None:\n",
    "        if isinstance(spectrogram, torch.Tensor):\n",
    "            spectrogram = spectrogram.to('cpu').numpy()\n",
    "        if len(spectrogram.shape) == 3:\n",
    "            spectrogram = spectrogram[0]\n",
    "    if isinstance(audio, torch.Tensor):\n",
    "        audio = audio.to('cpu').numpy()\n",
    "    return spectrogram, audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3e2c2d-d164-4725-b159-2bbf268182fb",
   "metadata": {
    "id": "8901f88b"
   },
   "outputs": [],
   "source": [
    "last_ckpt = str(ls(artifact_dir)[0])\n",
    "print(last_ckpt)\n",
    "\n",
    "spec_model = FastPitchModel.load_from_checkpoint(last_ckpt)\n",
    "spec_model.eval().cuda();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ec1b18-11e1-419e-8e35-da71de9ff752",
   "metadata": {},
   "source": [
    "### View results in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be1a2ba-ddad-4df0-a510-76fa6b23d5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = pd.read_json(f\"{SPEAKER_ID}_manifest_valid_local.json\", lines=True)\n",
    "valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3851c08c-e72d-49af-b51e-56902165a922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_audio(text):\n",
    "    \"Generate MEL and Synth Audio\"\n",
    "    spec, audio = infer(spec_model, vocoder, text, speaker=speaker_id)\n",
    "    return spec, audio.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6810c3-2d73-485e-9730-52ee64867bdb",
   "metadata": {
    "id": "8901f88b"
   },
   "outputs": [],
   "source": [
    "for _, val_record in valid_df.iterrows():\n",
    "    print(\"Real validation audio\")\n",
    "    ipd.display(ipd.Audio(val_record['audio_filepath'], rate=22050))\n",
    "    print(f\"SYNTHESIZED FOR -- Speaker: {new_speaker_id} | Dataset size: {duration_mins} mins | Mixing:{mixing} | Text: {val_record['text']}\")\n",
    "    spec, audio = generate_audio(val_record[\"text\"])\n",
    "    ipd.display(ipd.Audio(audio, rate=22050))\n",
    "    %matplotlib inline\n",
    "    imshow(spec, origin=\"lower\", aspect=\"auto\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3753d143-b2c8-420c-9d8f-31cb59c03546",
   "metadata": {},
   "source": [
    "### The same on a wandb.Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4306977-f04d-46da-92ea-a7201eb4d6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = wandb.Table(columns=['Text', 'Real validation audio', f'Audio Speaker {new_speaker_id}', 'Spec'])\n",
    "\n",
    "sample_rate=22050\n",
    "\n",
    "for _, val_record in valid_df.iterrows():\n",
    "    speaker_spec, speaker_audio = infer(spec_model, vocoder, val_record['text'], speaker=speaker_id)\n",
    "    row = [val_record[\"text_no_preprocessing\"],\n",
    "           wandb.Audio(val_record['audio_filepath'], sample_rate=sample_rate), \n",
    "           wandb.Audio(speaker_audio.flatten(), sample_rate=sample_rate),\n",
    "           wandb.Image(speaker_spec)]\n",
    "    table.add_data(*row)\n",
    "\n",
    "wandb.log({\"table\": table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2bdd02-66e8-43c7-9343-012f7e157d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18edee44-c23e-48b9-9816-c5f2ba1d6085",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
