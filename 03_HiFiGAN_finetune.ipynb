{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28454638",
   "metadata": {
    "id": "ge2s7s9-w3py"
   },
   "source": [
    "## Improving Speech Quality\n",
    "\n",
    "We see that from fine-tuning FastPitch, we were able to generate audio in a male voice but the audio quality is not as good as we expect. We recommend two steps to improve audio quality:\n",
    "\n",
    "* Finetuning HiFi-GAN\n",
    "* Adding more data\n",
    "\n",
    "We'll focus on the former for this workshop.\n",
    "\n",
    "### Finetuning HiFi-GAN\n",
    "From the synthesized samples, there might be audible audio crackling. To fix this, we need to finetune HiFi-GAN on the new speaker's data. HiFi-GAN shows improvement using **synthesized mel spectrograms**, so the first step is to generate mel spectrograms with our finetuned FastPitch model to use as input.\n",
    "\n",
    "The code below uses our finetuned model to generate synthesized mels for the training and validation sets we have been using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import soundfile as sf\n",
    "\n",
    "import IPython.display as ipd\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from nemo.collections.tts.torch.helpers import BetaBinomialInterpolator\n",
    "\n",
    "from nemo.collections.tts.models import FastPitchModel\n",
    "\n",
    "from nemo.collections.tts.models import HifiGanModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload our fine-tuned FastPitch spectrogram generation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_ckpt_from_last_run(\n",
    "        base_dir, \n",
    "        new_speaker_id, \n",
    "        duration_mins, \n",
    "        mixing_enabled, \n",
    "        original_speaker_id, \n",
    "        model_name=\"FastPitch\"\n",
    "    ):    \n",
    "    mixing = \"no_mixing\" if not mixing_enabled else \"mixing\"\n",
    "    \n",
    "    d = f\"{original_speaker_id}_to_{new_speaker_id}_{mixing}_{duration_mins}_mins\"\n",
    "    \n",
    "    exp_dirs = list([i for i in (Path(base_dir) / d / model_name).iterdir() if i.is_dir()])\n",
    "    last_exp_dir = sorted(exp_dirs)[-1]\n",
    "    \n",
    "    last_checkpoint_dir = last_exp_dir / \"checkpoints\"\n",
    "    \n",
    "    last_ckpt = list(last_checkpoint_dir.glob('*-last.ckpt'))\n",
    "\n",
    "    if len(last_ckpt) == 0:\n",
    "        raise ValueError(f\"There is no last checkpoint in {last_checkpoint_dir}.\")\n",
    "    \n",
    "    return str(last_ckpt[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_speaker_id = 9017\n",
    "duration_mins = 5\n",
    "mixing = False\n",
    "original_speaker_id = \"ljspeech\"\n",
    "\n",
    "last_ckpt = get_best_ckpt_from_last_run(\"./\", new_speaker_id, duration_mins, mixing, original_speaker_id)\n",
    "print(last_ckpt)\n",
    "\n",
    "spec_model = FastPitchModel.load_from_checkpoint(last_ckpt)\n",
    "spec_model.eval().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function for loading and transcribing `.wav` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wav(audio_file, target_sr=None):\n",
    "    with sf.SoundFile(audio_file, 'r') as f:\n",
    "        samples = f.read(dtype='float32')\n",
    "        sample_rate = f.samplerate\n",
    "        if target_sr is not None and target_sr != sample_rate:\n",
    "            samples = librosa.core.resample(samples, orig_sr=sample_rate, target_sr=target_sr)\n",
    "    return samples.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This helper function takes a FastPitch manifest file (located at `manifest_path`), uses the spectrogram model (`spec_model`) to generate spectrograms and store them in `mel_dir`, then generates an associated HiFi-GAN manifest file at `hifigan_manifest_path`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_spectrograms(manifest_path, mel_dir, hifigan_manifest_path, spec_model=spec_model):\n",
    "\n",
    "    # Get records from the training manifest (at manifest_path)\n",
    "    records = []\n",
    "    with open(manifest_path, \"r\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            records.append(json.loads(line))\n",
    "\n",
    "    beta_binomial_interpolator = BetaBinomialInterpolator()\n",
    "    spec_model.eval()\n",
    "\n",
    "    device = spec_model.device\n",
    "\n",
    "    save_dir = Path(mel_dir)\n",
    "    save_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    # Generate spectrograms (we need to use ground truth alignment for correct matching between audio and mels)\n",
    "    for i, r in enumerate(records):\n",
    "        audio = load_wav(r[\"audio_filepath\"])\n",
    "        audio = torch.from_numpy(audio).unsqueeze(0).to(device)\n",
    "        audio_len = torch.tensor(audio.shape[1], dtype=torch.long, device=device).unsqueeze(0)\n",
    "        \n",
    "        # Again, our finetuned FastPitch model doesn't use multiple speakers,\n",
    "        # but we keep the code to support it here for reference\n",
    "        if spec_model.fastpitch.speaker_emb is not None and \"speaker\" in r:\n",
    "            speaker = torch.tensor([r['speaker']]).to(device)\n",
    "        else:\n",
    "            speaker = None\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if \"normalized_text\" in r:\n",
    "                text = spec_model.parse(r[\"normalized_text\"], normalize=False)\n",
    "            else:\n",
    "                text = spec_model.parse(r['text'])\n",
    "            \n",
    "            text_len = torch.tensor(text.shape[-1], dtype=torch.long, device=device).unsqueeze(0)\n",
    "        \n",
    "            spect, spect_len = spec_model.preprocessor(input_signal=audio, length=audio_len)\n",
    "\n",
    "            # Generate attention prior and spectrogram inputs for HiFi-GAN\n",
    "            attn_prior = torch.from_numpy(\n",
    "            beta_binomial_interpolator(spect_len.item(), text_len.item())\n",
    "            ).unsqueeze(0).to(text.device)\n",
    "                \n",
    "            spectrogram = spec_model.forward(\n",
    "            text=text, \n",
    "            input_lens=text_len, \n",
    "            spec=spect, \n",
    "            mel_lens=spect_len, \n",
    "            attn_prior=attn_prior,\n",
    "            speaker=speaker,\n",
    "            )[0]\n",
    "            \n",
    "            save_path = save_dir / f\"mel_{i}.npy\"\n",
    "            np.save(save_path, spectrogram[0].to('cpu').numpy())\n",
    "            r[\"mel_filepath\"] = str(save_path)\n",
    "\n",
    "    hifigan_manifest_path = \"hifigan_train_ft.json\"\n",
    "    with open(hifigan_manifest_path, \"w\") as f:\n",
    "        for r in records:\n",
    "            f.write(json.dumps(r) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the mel-spectrograms and HiFi-GAN manifest files for the training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"train\"\n",
    "manifest_path = f\"{new_speaker_id}_manifest_{dataset}_dur_{duration_mins}_mins_local.json\"\n",
    "mel_dir = f\"{new_speaker_id}_manifest_{dataset}_dur_{duration_mins}_mins_local_mels\"\n",
    "hifigan_manifest_path = f\"hifigan_{dataset}_ft.json\"\n",
    "generate_spectrograms(manifest_path, mel_dir, hifigan_manifest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"val\"\n",
    "manifest_path = f\"{new_speaker_id}_manifest_dev_ns_all_local.json\"\n",
    "mel_dir = f\"{new_speaker_id}_manifest_{dataset}_ns_all_local_mels\"\n",
    "hifigan_manifest_path = f\"hifigan_{dataset}_ft.json\"\n",
    "generate_spectrograms(manifest_path, mel_dir, hifigan_manifest_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tune HiFi-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python hifigan_finetune.py \\\n",
    "    --config-name=hifigan.yaml \\\n",
    "    model.max_steps=1000 \\\n",
    "    ~model.optim.sched \\\n",
    "    train_dataset=./hifigan_train_ft.json \\\n",
    "    validation_datasets=./hifigan_val_ft.json \\\n",
    "    exp_manager.exp_dir=./hifigan_finetune \\\n",
    "    +exp_manager.create_wandb_logger=True \\\n",
    "    +exp_manager.wandb_logger_kwargs='{project:tts-workshop, job_type:training, log_model:True}' \\\n",
    "    +init_from_pretrained_model=tts_hifigan "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the best checkpoint from the latest HiFi-GAN fine-tuning run and use it to generate a synthesized voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_ckpt_from_last_hifigan_run(base_dir):    \n",
    "    d = f\"hifigan_ft/HifiGan\"    \n",
    "    exp_dirs = list([i for i in (Path(base_dir) / d ).iterdir() if i.is_dir()])\n",
    "    last_exp_dir = sorted(exp_dirs)[-1]\n",
    "    \n",
    "    last_checkpoint_dir = last_exp_dir / \"checkpoints\"\n",
    "    \n",
    "    last_ckpt = list(last_checkpoint_dir.glob('*-last.ckpt'))\n",
    "\n",
    "    if len(last_ckpt) == 0:\n",
    "        raise ValueError(f\"There is no last checkpoint in {last_checkpoint_dir}.\")\n",
    "    \n",
    "    return str(last_ckpt[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hifigan_ckpt = get_best_ckpt_from_last_hifigan_run(\"./\")\n",
    "\n",
    "vocoder_model = HifiGanModel.load_from_checkpoint(last_hifigan_ckpt)\n",
    "vocoder_model.eval().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(spec_gen_model, vocoder_model, str_input, speaker=None):\n",
    "    \"\"\"\n",
    "    Synthesizes spectrogram and audio from a text string given a spectrogram synthesis and vocoder model.\n",
    "    \n",
    "    Args:\n",
    "        spec_gen_model: Spectrogram generator model (FastPitch in our case)\n",
    "        vocoder_model: Vocoder model (HiFiGAN in our case)\n",
    "        str_input: Text input for the synthesis\n",
    "        speaker: Speaker ID\n",
    "    \n",
    "    Returns:\n",
    "        spectrogram and waveform of the synthesized audio.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        parsed = spec_gen_model.parse(str_input)\n",
    "        if speaker is not None:\n",
    "            speaker = torch.tensor([speaker]).long().to(device=spec_gen_model.device)\n",
    "        spectrogram = spec_gen_model.generate_spectrogram(tokens=parsed, speaker=speaker)\n",
    "        audio = vocoder_model.convert_spectrogram_to_audio(spec=spectrogram)\n",
    "        \n",
    "    if spectrogram is not None:\n",
    "        if isinstance(spectrogram, torch.Tensor):\n",
    "            spectrogram = spectrogram.to('cpu').numpy()\n",
    "        if len(spectrogram.shape) == 3:\n",
    "            spectrogram = spectrogram[0]\n",
    "    if isinstance(audio, torch.Tensor):\n",
    "        audio = audio.to('cpu').numpy()\n",
    "    return spectrogram, audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_val = 2  # Number of validation samples\n",
    "val_records = []\n",
    "with open(f\"hifigan_val_ft.json\", \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        val_records.append(json.loads(line))\n",
    "        if len(val_records) >= num_val:\n",
    "            break\n",
    "            \n",
    "for val_record in val_records:\n",
    "    print(\"Real validation audio\")\n",
    "    ipd.display(ipd.Audio(val_record['audio_filepath'], rate=22050))\n",
    "    print(f\"SYNTHESIZED FOR -- Speaker: {new_speaker_id} | Dataset size: {duration_mins} mins | Mixing:{mixing} | Text: {val_record['text']}\")\n",
    "    spec, audio = infer(spec_model, vocoder_model, val_record['text'], speaker=speaker_id)\n",
    "    ipd.display(ipd.Audio(audio, rate=22050))\n",
    "    %matplotlib inline\n",
    "    imshow(spec, origin=\"lower\", aspect=\"auto\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
